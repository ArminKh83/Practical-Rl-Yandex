{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE in PyTorch\n",
    "\n",
    "Just like we did before for Q-learning, this time we'll design a PyTorch network to learn `CartPole-v0` via policy gradient (REINFORCE).\n",
    "\n",
    "Most of the code in this notebook is taken from approximate Q-learning, so you'll find it more or less familiar and even simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
    "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
    "    !pip install -q gymnasium\n",
    "    !pip install moviepy\n",
    "    !apt install ffmpeg\n",
    "    !pip install imageio-ffmpeg\n",
    "    !touch .setup_complete\n",
    "\n",
    "# This code creates a virtual display to draw game images on.\n",
    "# It will have no effect if your machine has a monitor.\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    os.environ['DISPLAY'] = ':1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T09:41:48.793310Z",
     "start_time": "2024-08-05T09:41:48.263936Z"
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T09:41:48.795551Z",
     "start_time": "2024-08-05T09:41:48.794018Z"
    }
   },
   "outputs": [],
   "source": [
    "# also you need to install ffmpeg if not installed\n",
    "# for MacOS: ! brew install ffmpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A caveat: with some versions of `pyglet`, the following cell may crash with `NameError: name 'base' is not defined`. The corresponding bug report is [here](https://github.com/pyglet/pyglet/issues/134). If you see this error, try restarting the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T09:49:51.488186Z",
     "start_time": "2024-08-05T09:49:51.396988Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "(4,)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x7132651f2410>"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "\n",
    "# gym compatibility: unwrap TimeLimit\n",
    "if hasattr(env, '_max_episode_steps'):\n",
    "    env = env.env\n",
    "\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "print(n_actions)\n",
    "print(state_dim)\n",
    "\n",
    "plt.imshow(env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the network for REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n",
    "\n",
    "For numerical stability, please __do not include the softmax layer into your network architecture__.\n",
    "We'll use softmax or log-softmax where appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T09:41:56.407390Z",
     "start_time": "2024-08-05T09:41:55.251772Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T09:44:12.118756Z",
     "start_time": "2024-08-05T09:44:12.106644Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build a simple neural network that predicts policy logits. \n",
    "# Keep it simple: CartPole isn't worth deep architectures.\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(*state_dim, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, n_actions)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: output value of this function is not a torch tensor, it's a numpy array.\n",
    "So, here gradient calculation is not needed.\n",
    "<br>\n",
    "Use [no_grad](https://pytorch.org/docs/stable/autograd.html#torch.autograd.no_grad)\n",
    "to suppress gradient calculation.\n",
    "<br>\n",
    "Also, `.detach()` (or legacy `.data` property) can be used instead, but there is a difference:\n",
    "<br>\n",
    "With `.detach()` computational graph is built but then disconnected from a particular tensor,\n",
    "so `.detach()` should be used if that graph is needed for backprop via some other (not detached) tensor;\n",
    "<br>\n",
    "In contrast, no graph is built by any operation in `no_grad()` context, thus it's preferable here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T09:53:46.416693Z",
     "start_time": "2024-08-05T09:53:46.413397Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_probs(states):\n",
    "    \"\"\" \n",
    "    Predict action probabilities given states.\n",
    "    :param states: numpy array of shape [batch, state_shape]\n",
    "    :returns: numpy array of shape [batch, n_actions]\n",
    "    \"\"\"\n",
    "    # convert states, compute logits, use softmax to get probability\n",
    "    torch_tensors = torch.tensor(states, dtype=torch.float32)\n",
    "    logits = model(torch_tensors)\n",
    "    # print(logits.size())\n",
    "\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    # print(probs.size())\n",
    "    return probs.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T09:53:51.358280Z",
     "start_time": "2024-08-05T09:53:51.354493Z"
    }
   },
   "outputs": [],
   "source": [
    "test_states = np.array([env.reset()[0] for _ in range(5)])\n",
    "test_probas = predict_probs(test_states)\n",
    "\n",
    "assert isinstance(test_probas, np.ndarray), \\\n",
    "    \"you must return np array and not %s\" % type(test_probas)\n",
    "assert tuple(test_probas.shape) == (test_states.shape[0], env.action_space.n), \\\n",
    "    \"wrong output shape: %s\" % np.shape(test_probas)\n",
    "assert np.allclose(np.sum(test_probas, axis=1), 1), \"probabilities do not sum to 1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play the game\n",
    "\n",
    "We can now use our newly built agent to play the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T09:54:56.103032Z",
     "start_time": "2024-08-05T09:54:56.100120Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_session(env, t_max=1000):\n",
    "    \"\"\"\n",
    "    Play a full session with REINFORCE agent.\n",
    "    Returns sequences of states, actions, and rewards.\n",
    "    \"\"\"\n",
    "    # arrays to record session\n",
    "    states, actions, rewards = [], [], []\n",
    "\n",
    "    s = env.reset()[0]\n",
    "\n",
    "    for t in range(t_max):\n",
    "        # action probabilities array aka pi(a|s)\n",
    "        action_probs = predict_probs(np.array([s]))[0]\n",
    "\n",
    "        # Sample action with given probabilities.\n",
    "        a = np.random.choice(range(n_actions), p=action_probs)\n",
    "\n",
    "        new_s, r, terminated, truncated, info = env.step(a)\n",
    "\n",
    "        # record session history to train later\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "\n",
    "        s = new_s\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    return states, actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T09:55:03.768195Z",
     "start_time": "2024-08-05T09:55:03.763111Z"
    }
   },
   "outputs": [],
   "source": [
    "# test it\n",
    "states, actions, rewards = generate_session(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing cumulative rewards\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "G_t &= r_t + \\gamma r_{t + 1} + \\gamma^2 r_{t + 2} + \\ldots \\\\\n",
    "&= \\sum_{i = t}^T \\gamma^{i - t} r_i \\\\\n",
    "&= r_t + \\gamma * G_{t + 1}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T10:02:27.718618Z",
     "start_time": "2024-08-05T10:02:27.713397Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_cumulative_rewards(rewards,  # rewards at each step\n",
    "                           gamma=0.99  # discount for reward\n",
    "                           ):\n",
    "    \"\"\"\n",
    "    Take a list of immediate rewards r(s,a) for the whole session \n",
    "    and compute cumulative returns (a.k.a. G(s,a) in Sutton '16).\n",
    "    \n",
    "    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
    "\n",
    "    A simple way to compute cumulative rewards is to iterate from the last\n",
    "    to the first timestep and compute G_t = r_t + gamma*G_{t+1} recurrently\n",
    "\n",
    "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
    "    \"\"\"\n",
    "\n",
    "    cumulative_rewards = []\n",
    "    R = 0\n",
    "\n",
    "    for reward in reversed(rewards):\n",
    "        R = reward + gamma * R\n",
    "        cumulative_rewards.append(R)\n",
    "    cumulative_rewards.reverse()\n",
    "    return np.array(cumulative_rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T10:02:28.286585Z",
     "start_time": "2024-08-05T10:02:28.281268Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looks good!\n"
     ]
    }
   ],
   "source": [
    "get_cumulative_rewards(rewards)\n",
    "assert len(get_cumulative_rewards(list(range(100)))) == 100\n",
    "assert np.allclose(\n",
    "    get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9),\n",
    "    [1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
    "assert np.allclose(\n",
    "    get_cumulative_rewards([0, 0, 1, -2, 3, -4, 0], gamma=0.5),\n",
    "    [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
    "assert np.allclose(\n",
    "    get_cumulative_rewards([0, 0, 1, 2, 3, 4, 0], gamma=0),\n",
    "    [0, 0, 1, 2, 3, 4, 0])\n",
    "print(\"looks good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function and updates\n",
    "\n",
    "We now need to define objective and update over policy gradient.\n",
    "\n",
    "Our objective function is\n",
    "\n",
    "$$ J \\approx  { 1 \\over N } \\sum_{s_i,a_i} G(s_i,a_i) $$\n",
    "\n",
    "REINFORCE defines a way to compute the gradient of the expected reward with respect to policy parameters. The formula is as follows:\n",
    "\n",
    "$$ \\nabla_\\theta \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\nabla_\\theta \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
    "\n",
    "We can abuse PyTorch's capabilities for automatic differentiation by defining our objective function as follows:\n",
    "\n",
    "$$ \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
    "\n",
    "When you compute the gradient of that function with respect to network weights $\\theta$, it will become exactly the policy gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T10:18:21.418594Z",
     "start_time": "2024-08-05T10:18:21.411569Z"
    }
   },
   "outputs": [],
   "source": [
    "# Your code: define optimizers\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n",
    "\n",
    "\n",
    "def train_on_session(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):\n",
    "    \"\"\"\n",
    "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
    "    Updates agent's weights by following the policy gradient above.\n",
    "    Please use Adam optimizer with default parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    # cast everything into torch tensors\n",
    "    states = torch.tensor(states, dtype=torch.float32)\n",
    "    actions = torch.tensor(actions, dtype=torch.int64)\n",
    "    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n",
    "    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32)\n",
    "\n",
    "    # predict logits, probas and log-probas using an agent.\n",
    "    logits = model(states)\n",
    "    probs = nn.functional.softmax(logits, -1)\n",
    "    log_probs = nn.functional.log_softmax(logits, -1)\n",
    "\n",
    "    assert all(isinstance(v, torch.Tensor) for v in [logits, probs, log_probs]), \\\n",
    "        \"please use compute using torch tensors and don't use predict_probs function\"\n",
    "\n",
    "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
    "    log_probs_for_actions = torch.sum(\n",
    "        log_probs * F.one_hot(actions, env.action_space.n), dim=1)\n",
    "\n",
    "    # Compute loss here. Don't forgen entropy regularization with `entropy_coef` \n",
    "    entropy = -torch.sum(probs * log_probs, dim=1).mean()\n",
    "    loss = - torch.mean(log_probs_for_actions * cumulative_returns) - entropy_coef * entropy\n",
    "\n",
    "    # Gradient descent step\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # technical: return session rewards to print them later\n",
    "    return np.sum(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T10:18:39.629947Z",
     "start_time": "2024-08-05T10:18:22.956003Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14359/2649682976.py:13: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  states = torch.tensor(states, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward: 28.410\n",
      "mean reward: 42.760\n",
      "mean reward: 66.760\n",
      "mean reward: 129.900\n",
      "mean reward: 103.370\n",
      "mean reward: 147.330\n",
      "mean reward: 143.440\n",
      "mean reward: 121.180\n",
      "mean reward: 161.840\n",
      "mean reward: 330.480\n",
      "mean reward: 376.650\n",
      "mean reward: 637.260\n",
      "You Win!\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    rewards = [train_on_session(*generate_session(env)) for _ in range(100)]  # generate new sessions\n",
    "\n",
    "    print(\"mean reward: %.3f\" % (np.mean(rewards)))\n",
    "\n",
    "    if np.mean(rewards) > 500:\n",
    "        print(\"You Win!\")  # but you can train even further\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results & video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T10:20:46.257714Z",
     "start_time": "2024-08-05T10:20:43.044760Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /home/armin/D/Code/Python/NoteBooks/Notebooks/RL/Practical RL/Practical_RL-master/week06_policy_based/videos/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /home/armin/D/Code/Python/NoteBooks/Notebooks/RL/Practical RL/Practical_RL-master/week06_policy_based/videos/rl-video-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/armin/D/Code/Python/NoteBooks/Notebooks/RL/Practical RL/Practical_RL-master/week06_policy_based/videos/rl-video-episode-0.mp4\n",
      "Moviepy - Building video /home/armin/D/Code/Python/NoteBooks/Notebooks/RL/Practical RL/Practical_RL-master/week06_policy_based/videos/rl-video-episode-1.mp4.\n",
      "Moviepy - Writing video /home/armin/D/Code/Python/NoteBooks/Notebooks/RL/Practical RL/Practical_RL-master/week06_policy_based/videos/rl-video-episode-1.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/armin/D/Code/Python/NoteBooks/Notebooks/RL/Practical RL/Practical_RL-master/week06_policy_based/videos/rl-video-episode-1.mp4\n",
      "Moviepy - Building video /home/armin/D/Code/Python/NoteBooks/Notebooks/RL/Practical RL/Practical_RL-master/week06_policy_based/videos/rl-video-episode-8.mp4.\n",
      "Moviepy - Writing video /home/armin/D/Code/Python/NoteBooks/Notebooks/RL/Practical RL/Practical_RL-master/week06_policy_based/videos/rl-video-episode-8.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/armin/D/Code/Python/NoteBooks/Notebooks/RL/Practical RL/Practical_RL-master/week06_policy_based/videos/rl-video-episode-8.mp4\n"
     ]
    }
   ],
   "source": [
    "# Record sessions\n",
    "\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "with gym.make(\"CartPole-v1\", render_mode=\"rgb_array\") as env, RecordVideo(\n",
    "        env=env, video_folder=\"./videos\"\n",
    ") as env_monitor:\n",
    "    sessions = [generate_session(env_monitor) for _ in range(10)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T10:21:16.381674Z",
     "start_time": "2024-08-05T10:21:16.376729Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n<video width=\"640\" height=\"480\" controls>\n  <source src=\"videos/rl-video-episode-8.mp4\" type=\"video/mp4\">\n</video>\n"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show video. This may not work in some setups. If it doesn't\n",
    "# work for you, you can download the videos and view them locally.\n",
    "\n",
    "from pathlib import Path\n",
    "from base64 import b64encode\n",
    "from IPython.display import HTML\n",
    "\n",
    "video_paths = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n",
    "video_path = video_paths[-1]  # You can also try other indices\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    # https://stackoverflow.com/a/57378660/1214547\n",
    "    with video_path.open('rb') as fp:\n",
    "        mp4 = fp.read()\n",
    "    data_url = 'data:video/mp4;base64,' + b64encode(mp4).decode()\n",
    "else:\n",
    "    data_url = str(video_path)\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(data_url))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
